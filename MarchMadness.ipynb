{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# March Madness Predictions using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KenPom summary data\n",
    "kenpom_summary = pd.read_csv(\"./data/archive/INT _ KenPom _ Summary.csv\")\n",
    "kenpom_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load efficiency data\n",
    "kenpom_efficiency = pd.read_csv(\"./data/archive/INT _ KenPom _ Efficiency.csv\")\n",
    "kenpom_efficiency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load defensive data\n",
    "kenpom_defense = pd.read_csv(\"./data/archive/INT _ KenPom _ Defense.csv\")\n",
    "kenpom_defense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load offensive data\n",
    "kenpom_offense = pd.read_csv(\"./data/archive/INT _ KenPom _ Offense.csv\")\n",
    "kenpom_offense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load height data\n",
    "kenpom_height = pd.read_csv(\"./data/archive/INT _ KenPom _ Height.csv\")\n",
    "kenpom_height.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load March Madness games\n",
    "march_madness = pd.read_csv(\"./data/archive/DEV _ March Madness.csv\")\n",
    "march_madness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tournament teams data\n",
    "tournament_teams = pd.read_csv(\"./data/archive/REF _ Post-Season Tournament Teams.csv\")\n",
    "tournament_teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets to create a comprehensive team profile dataset\n",
    "def merge_team_data(year):\n",
    "    # Filter datasets for the specified year\n",
    "    summary = kenpom_summary[kenpom_summary['Season'] == year]\n",
    "    efficiency = kenpom_efficiency[kenpom_efficiency['Season'] == year]\n",
    "    defense = kenpom_defense[kenpom_defense['Season'] == year]\n",
    "    offense = kenpom_offense[kenpom_offense['Season'] == year]\n",
    "    height = kenpom_height[kenpom_height['Season'] == year]\n",
    "    \n",
    "    # Merge on Team column\n",
    "    merged = summary.merge(efficiency, on=['Season', 'TeamName'], how='inner')\n",
    "    merged = merged.merge(defense, on=['Season', 'TeamName'], how='inner')\n",
    "    merged = merged.merge(offense, on=['Season', 'TeamName'], how='inner')\n",
    "    merged = merged.merge(height, on=['Season', 'TeamName'], how='inner')\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Create a dataset for a recent year (adjust as needed)\n",
    "team_data_2023 = merge_team_data(2023)\n",
    "team_data_2023.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all team matchups from tournament data\n",
    "tournament_matchups = pd.read_csv(\"./data/archive/DEV _ March Madness.csv\")\n",
    "tournament_matchups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with matchups and feature differences for each game\n",
    "def create_matchup_features(team1, team2, team_data):\n",
    "    # Get data for both teams\n",
    "    team1_data = team_data[team_data['TeamName'] == team1]\n",
    "    team2_data = team_data[team_data['TeamName'] == team2]\n",
    "    \n",
    "    if team1_data.empty or team2_data.empty:\n",
    "        return None\n",
    "    \n",
    "    # Select important features\n",
    "    features = [\n",
    "        'AdjEM', 'AdjOE', 'AdjDE', 'Tempo', 'eFGPct', 'TOPct', 'ORPct', 'FTRate',\n",
    "        'BlockPct', 'StlRate', 'Experience', 'AvgHeight', 'Bench'\n",
    "    ]\n",
    "    \n",
    "    # Create differentials for each feature\n",
    "    diff_features = {}\n",
    "    for feature in features:\n",
    "        if feature in team1_data.columns and feature in team2_data.columns:\n",
    "            diff_features[f\"{feature}_DIFF\"] = float(team1_data[feature].values[0]) - float(team2_data[feature].values[0])\n",
    "    \n",
    "    # Add team info\n",
    "    diff_features['TEAM1'] = team1\n",
    "    diff_features['TEAM2'] = team2\n",
    "    \n",
    "    return diff_features\n",
    "\n",
    "# Example of creating features for a single matchup\n",
    "sample_matchup = create_matchup_features('Duke', 'North Carolina', team_data_2023)\n",
    "sample_matchup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tournament results to create a labeled dataset\n",
    "def create_training_data(years):\n",
    "    all_matchups = []\n",
    "    \n",
    "    for year in years:\n",
    "        # Get team data for this year\n",
    "        team_data = merge_team_data(year)\n",
    "        \n",
    "        # Get tournament games for this year\n",
    "        tournament = tournament_teams[tournament_teams['Season'] == year]\n",
    "        \n",
    "        # For each tournament game, create features\n",
    "        # This would require tournament game results which should be in your dataset\n",
    "        # We'll simulate it here\n",
    "        for idx, game in tournament.iterrows():\n",
    "            # In a real dataset, you'd have actual matchups and results\n",
    "            # Here we'll need to modify based on your actual data structure\n",
    "            pass\n",
    "    \n",
    "    return pd.DataFrame(all_matchups)\n",
    "\n",
    "# Training data would be created based on historical tournament matchups\n",
    "# For now, we'll generate synthetic data for demonstration\n",
    "def generate_synthetic_training_data(n_samples=1000):\n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Create synthetic feature differences\n",
    "    for _ in range(n_samples):\n",
    "        sample = {\n",
    "            'AdjEM_DIFF': np.random.normal(0, 10),\n",
    "            'AdjOE_DIFF': np.random.normal(0, 5),\n",
    "            'AdjDE_DIFF': np.random.normal(0, 5),\n",
    "            'Tempo_DIFF': np.random.normal(0, 3),\n",
    "            'eFGPct_DIFF': np.random.normal(0, 0.05),\n",
    "            'TOPct_DIFF': np.random.normal(0, 0.02),\n",
    "            'ORPct_DIFF': np.random.normal(0, 0.03),\n",
    "            'FTRate_DIFF': np.random.normal(0, 0.04),\n",
    "            'BlockPct_DIFF': np.random.normal(0, 0.02),\n",
    "            'StlRate_DIFF': np.random.normal(0, 0.01),\n",
    "            'Experience_DIFF': np.random.normal(0, 0.5),\n",
    "            'AvgHeight_DIFF': np.random.normal(0, 1),\n",
    "            'Bench_DIFF': np.random.normal(0, 5),\n",
    "        }\n",
    "        \n",
    "        # Generate result based on feature differences\n",
    "        # Team 1 wins if AdjEM_DIFF > 0 with some randomness\n",
    "        prob_team1_wins = 1 / (1 + np.exp(-0.1 * sample['AdjEM_DIFF']))\n",
    "        sample['RESULT'] = 1 if np.random.random() < prob_team1_wins else -1\n",
    "        \n",
    "        synthetic_data.append(sample)\n",
    "    \n",
    "    return pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Generate synthetic training data\n",
    "training_data = generate_synthetic_training_data(2000)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = training_data.drop('RESULT', axis=1)\n",
    "y = training_data['RESULT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Random Forest using our own implementation\n",
    "from RandomForest.RandomForest import RandomForest\n",
    "\n",
    "# Create numpy arrays from training data\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "train_data_np = np.hstack((X_train_np, y_train_np))\n",
    "\n",
    "# Initialize and train our custom Random Forest\n",
    "n_features = X_train.shape[1]\n",
    "my_rf = RandomForest(n_features=int(n_features * 0.7), n_estimators=100, tree_params=dict(max_depth=10, min_samples_split=5))\n",
    "my_rf.build_forest(train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Random Forest\n",
    "sklearn_rf = RandomForestClassifier(n_estimators=500, max_depth=10, max_features=\"sqrt\", random_state=42)\n",
    "sklearn_rf.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Neural Network\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define and train the neural network\n",
    "nn_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(X_train_scaled, (y_train == 1).astype(int), epochs=30, batch_size=32, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for custom RF\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "test_data_np = np.hstack((X_test_np, y_test_np))\n",
    "\n",
    "# Make predictions with custom RF\n",
    "my_rf_acc = my_rf.calculate_accuracy(test_data_np)\n",
    "print(f\"Custom Random Forest Accuracy: {my_rf_acc:.4f}\")\n",
    "\n",
    "# Make predictions with sklearn RF\n",
    "sklearn_rf_preds = sklearn_rf.predict(X_test)\n",
    "sklearn_rf_acc = metrics.accuracy_score(y_test, sklearn_rf_preds)\n",
    "print(f\"Scikit-learn Random Forest Accuracy: {sklearn_rf_acc:.4f}\")\n",
    "\n",
    "# Make predictions with XGBoost\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_acc = metrics.accuracy_score(y_test, xgb_preds)\n",
    "print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")\n",
    "\n",
    "# Make predictions with Neural Network\n",
    "nn_preds = (nn_model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "nn_acc = metrics.accuracy_score((y_test == 1).astype(int), nn_preds)\n",
    "print(f\"Neural Network Accuracy: {nn_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = pd.Series(sklearn_rf.feature_importances_, index=X.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate March Madness Bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate a game between two teams\n",
    "def predict_game(team1, team2, model, team_data, scaler=None):\n",
    "    # Create feature differences between teams\n",
    "    game_features = create_matchup_features(team1, team2, team_data)\n",
    "    \n",
    "    if game_features is None:\n",
    "        return None, 0.5\n",
    "    \n",
    "    # Extract features in the same order as training data\n",
    "    feature_values = [game_features[f] for f in X.columns]\n",
    "    features_df = pd.DataFrame([feature_values], columns=X.columns)\n",
    "    \n",
    "    # Scale features if using neural network\n",
    "    if scaler is not None:\n",
    "        features_scaled = scaler.transform(features_df)\n",
    "        prob = float(nn_model.predict(features_scaled)[0, 0])\n",
    "        prediction = 1 if prob > 0.5 else -1\n",
    "    else:\n",
    "        # For other models\n",
    "        prediction = model.predict(features_df)[0]\n",
    "        prob = float(model.predict_proba(features_df)[0, 1]) if hasattr(model, 'predict_proba') else 0.5\n",
    "    \n",
    "    # Return winner and probability\n",
    "    if prediction == 1:\n",
    "        return team1, prob\n",
    "    else:\n",
    "        return team2, 1 - prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2023 March Madness bracket (first round)\n",
    "team_data_2023 = merge_team_data(2023)\n",
    "bracket_2023 = [\n",
    "    # East region (just a few examples)\n",
    "    ('Purdue', 'Fairleigh Dickinson'),\n",
    "    ('Memphis', 'Florida Atlantic'),\n",
    "    ('Duke', 'Oral Roberts'),\n",
    "    ('Tennessee', 'Louisiana'),\n",
    "    \n",
    "    # Midwest region (just a few examples)\n",
    "    ('Houston', 'Northern Kentucky'),\n",
    "    ('Iowa', 'Auburn'), \n",
    "    ('Texas A&M', 'Penn State'),\n",
    "    ('Texas', 'Colgate'),\n",
    "    \n",
    "    # Add more matchups as needed\n",
    "]\n",
    "\n",
    "# Simulate the first round\n",
    "first_round_results = []\n",
    "for team1, team2 in bracket_2023:\n",
    "    winner, prob = predict_game(team1, team2, xgb_model, team_data_2023)\n",
    "    first_round_results.append((winner, prob))\n",
    "    print(f\"{team1} vs {team2} -> {winner} wins with {prob:.2f} probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate entire tournament\n",
    "def simulate_tournament(teams, model, team_data, scaler=None):\n",
    "    # Start with first round matchups\n",
    "    current_round = []\n",
    "    for i in range(0, len(teams), 2):\n",
    "        current_round.append((teams[i], teams[i+1]))\n",
    "    \n",
    "    # Keep track of the bracket\n",
    "    bracket = [current_round]\n",
    "    \n",
    "    # Simulate each round\n",
    "    while len(current_round) > 0:\n",
    "        next_round = []\n",
    "        for team1, team2 in current_round:\n",
    "            winner, prob = predict_game(team1, team2, model, team_data, scaler)\n",
    "            next_round.append(winner)\n",
    "        \n",
    "        # Match teams for next round\n",
    "        paired_next_round = []\n",
    "        for i in range(0, len(next_round), 2):\n",
    "            if i+1 < len(next_round):\n",
    "                paired_next_round.append((next_round[i], next_round[i+1]))\n",
    "        \n",
    "        current_round = paired_next_round\n",
    "        if len(current_round) > 0:\n",
    "            bracket.append(current_round)\n",
    "    \n",
    "    return bracket\n",
    "\n",
    "# Example simulation with a small subset of teams\n",
    "example_teams = [\n",
    "    'Gonzaga', 'Grand Canyon',\n",
    "    'Kansas', 'Howard',\n",
    "    'UCLA', 'UNC Asheville',\n",
    "    'Connecticut', 'Iona'\n",
    "]\n",
    "\n",
    "example_bracket = simulate_tournament(example_teams, xgb_model, team_data_2023)\n",
    "\n",
    "# Print bracket results\n",
    "for i, round_games in enumerate(example_bracket):\n",
    "    print(f\"\\nRound {i+1}:\")\n",
    "    for matchup in round_games:\n",
    "        if isinstance(matchup, tuple):\n",
    "            print(f\"{matchup[0]} vs {matchup[1]}\")\n",
    "        else:\n",
    "            print(f\"Champion: {matchup}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Team Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ELO-style ratings for teams\n",
    "def calculate_team_elos(matchups_data, init_elo=1500, k=32):\n",
    "    elo_ratings = defaultdict(lambda: init_elo)\n",
    "    \n",
    "    for _, row in matchups_data.iterrows():\n",
    "        team1 = row['TEAM1']\n",
    "        team2 = row['TEAM2']\n",
    "        result = row['RESULT']  # 1 if team1 won, -1 if team2 won\n",
    "        \n",
    "        # Get current ELOs\n",
    "        elo1 = elo_ratings[team1]\n",
    "        elo2 = elo_ratings[team2]\n",
    "        \n",
    "        # Calculate expected outcome\n",
    "        expected1 = 1 / (1 + 10 ** ((elo2 - elo1) / 400))\n",
    "        expected2 = 1 - expected1\n",
    "        \n",
    "        # Update ELOs based on result\n",
    "        if result == 1:  # team1 won\n",
    "            actual1, actual2 = 1, 0\n",
    "        else:  # team2 won\n",
    "            actual1, actual2 = 0, 1\n",
    "        \n",
    "        elo_ratings[team1] += k * (actual1 - expected1)\n",
    "        elo_ratings[team2] += k * (actual2 - expected2)\n",
    "    \n",
    "    return dict(elo_ratings)\n",
    "\n",
    "# We'd need real historical matchup data for this\n",
    "# For demonstration, we'll create a small synthetic dataset\n",
    "synthetic_matchups = pd.DataFrame({\n",
    "    'TEAM1': ['Duke', 'North Carolina', 'Kansas', 'Duke', 'Gonzaga'],\n",
    "    'TEAM2': ['Kentucky', 'Duke', 'North Carolina', 'Gonzaga', 'Kentucky'],\n",
    "    'RESULT': [1, -1, 1, 1, -1]  # 1 means TEAM1 won, -1 means TEAM2 won\n",
    "})\n",
    "\n",
    "# Calculate ELO ratings\n",
    "team_elos = calculate_team_elos(synthetic_matchups)\n",
    "print(\"Team ELO Ratings:\")\n",
    "for team, elo in sorted(team_elos.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{team}: {elo:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate upset probabilities\n",
    "def calculate_upset_probability(team1_seed, team2_seed, team1, team2, model, team_data, scaler=None):\n",
    "    # Define upset (lower seed beats higher seed)\n",
    "    lower_seed = max(team1_seed, team2_seed)\n",
    "    higher_seed = min(team1_seed, team2_seed)\n",
    "    lower_seed_team = team1 if team1_seed == lower_seed else team2\n",
    "    higher_seed_team = team1 if team1_seed == higher_seed else team2\n",
    "    \n",
    "    # Only consider potential upsets (difference in seeds)\n",
    "    if lower_seed - higher_seed < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Predict game\n",
    "    winner, prob = predict_game(higher_seed_team, lower_seed_team, model, team_data, scaler)\n",
    "    \n",
    "    # Probability of upset\n",
    "    upset_prob = 1 - prob if winner == higher_seed_team else prob\n",
    "    \n",
    "    return upset_prob\n",
    "\n",
    "# Example upset calculations\n",
    "upset_examples = [\n",
    "    (1, 16, 'Purdue', 'Fairleigh Dickinson'),\n",
    "    (4, 13, 'Virginia', 'Furman'),\n",
    "    (5, 12, 'San Diego State', 'Charleston'),\n",
    "    (2, 15, 'Arizona', 'Princeton')\n",
    "]\n",
    "\n",
    "for team1_seed, team2_seed, team1, team2 in upset_examples:\n",
    "    upset_prob = calculate_upset_probability(team1_seed, team2_seed, team1, team2, xgb_model, team_data_2023)\n",
    "    print(f\"Upset probability for #{team2_seed} {team2} over #{team1_seed} {team1}: {upset_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Tournament Bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple bracket visualization function\n",
    "def plot_bracket(bracket):\n",
    "    rounds = len(bracket)\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # No lines or ticks\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Plot each round\n",
    "    for r, round_games in enumerate(bracket):\n",
    "        x = r * 2\n",
    "        for i, matchup in enumerate(round_games):\n",
    "            # Calculate y position\n",
    "            num_games = len(round_games)\n",
    "            spacing = 8 / (num_games + 1)\n",
    "            y = i * spacing + spacing\n",
    "            \n",
    "            if isinstance(matchup, tuple):\n",
    "                team1, team2 = matchup\n",
    "                ax.text(x, y, f\"{team1} vs {team2}\", fontsize=10)\n",
    "            else:\n",
    "                ax.text(x, y, f\"Champion: {matchup}\", fontsize=12, weight='bold')\n",
    "    \n",
    "    ax.set_title(\"Tournament Bracket Simulation\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the example bracket\n",
    "plot_bracket(example_bracket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a full March Madness bracket\n",
    "def create_march_madness_bracket(year):\n",
    "    # This would require the actual tournament teams and seedings for the year\n",
    "    # For demonstration, we'll create a small example bracket\n",
    "    regions = {\n",
    "        'East': [\n",
    "            (1, 'Purdue'), (16, 'Fairleigh Dickinson'),\n",
    "            (8, 'Memphis'), (9, 'Florida Atlantic'),\n",
    "            (5, 'Duke'), (12, 'Oral Roberts'),\n",
    "            (4, 'Tennessee'), (13, 'Louisiana')\n",
    "        ],\n",
    "        'West': [\n",
    "            (1, 'Kansas'), (16, 'Howard'),\n",
    "            (8, 'Arkansas'), (9, 'Illinois'),\n",
    "            (5, 'Saint Mary\\'s'), (12, 'VCU'),\n",
    "            (4, 'Connecticut'), (13, 'Iona')\n",
    "        ],\n",
    "        'South': [\n",
    "            (1, 'Alabama'), (16, 'Texas A&M CC'),\n",
    "            (8, 'Maryland'), (9, 'West Virginia'),\n",
    "            (5, 'San Diego State'), (12, 'Charleston'),\n",
    "            (4, 'Virginia'), (13, 'Furman')\n",
    "        ],\n",
    "        'Midwest': [\n",
    "            (1, 'Houston'), (16, 'Northern Kentucky'),\n",
    "            (8, 'Iowa'), (9, 'Auburn'),\n",
    "            (5, 'Miami'), (12, 'Drake'),\n",
    "            (4, 'Indiana'), (13, 'Kent State')\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return regions\n",
    "\n",
    "# Function to simulate the entire tournament\n",
    "def simulate_full_tournament(regions, model, team_data, scaler=None):\n",
    "    region_results = {}\n",
    "    final_four = []\n",
    "    \n",
    "    # Simulate each region\n",
    "    for region_name, teams in regions.items():\n",
    "        # Extract team names in proper order\n",
    "        team_names = [team[1] for team in teams]\n",
    "        \n",
    "        # Simulate region bracket\n",
    "        region_bracket = simulate_tournament(team_names, model, team_data, scaler)\n",
    "        region_results[region_name] = region_bracket\n",
    "        \n",
    "        # Get regional champion\n",
    "        if len(region_bracket[-1]) == 1 and not isinstance(region_bracket[-1][0], tuple):\n",
    "            regional_champion = region_bracket[-1][0]\n",
    "            final_four.append((region_name, regional_champion))\n",
    "    \n",
    "    # Simulate Final Four\n",
    "    if len(final_four) == 4:\n",
    "        # Semifinal 1: East vs West\n",
    "        semi1_teams = [team for region, team in final_four if region in ['East', 'West']]\n",
    "        if len(semi1_teams) == 2:\n",
    "            semi1_winner, _ = predict_game(semi1_teams[0], semi1_teams[1], model, team_data, scaler)\n",
    "        else:\n",
    "            semi1_winner = None\n",
    "        \n",
    "        # Semifinal 2: South vs Midwest\n",
    "        semi2_teams = [team for region, team in final_four if region in ['South', 'Midwest']]\n",
    "        if len(semi2_teams) == 2:\n",
    "            semi2_winner, _ = predict_game(semi2_teams[0], semi2_teams[1], model, team_data, scaler)\n",
    "        else:\n",
    "            semi2_winner = None\n",
    "        \n",
    "        # Championship\n",
    "        if semi1_winner and semi2_winner:\n",
    "            champion, _ = predict_game(semi1_winner, semi2_winner, model, team_data, scaler)\n",
    "        else:\n",
    "            champion = None\n",
    "    else:\n",
    "        semi1_winner, semi2_winner, champion = None, None, None\n",
    "    \n",
    "    return {\n",
    "        'region_results': region_results,\n",
    "        'final_four': final_four,\n",
    "        'semifinal1': (semi1_teams[0], semi1_teams[1], semi1_winner) if 'semi1_teams' in locals() and len(semi1_teams) == 2 else None,\n",
    "        'semifinal2': (semi2_teams[0], semi2_teams[1], semi2_winner) if 'semi2_teams' in locals() and len(semi2_teams) == 2 else None,\n",
    "        'champion': champion\n",
    "    }\n",
    "\n",
    "# Simulate the tournament\n",
    "regions_2023 = create_march_madness_bracket(2023)\n",
    "tournament_result = simulate_full_tournament(regions_2023, xgb_model, team_data_2023)\n",
    "\n",
    "# Print the results\n",
    "print(\"Final Four Teams:\")\n",
    "for region, team in tournament_result['final_four']:\n",
    "    print(f\"{region}: {team}\")\n",
    "\n",
    "print(\"\\nChampionship Game:\")\n",
    "if tournament_result['semifinal1'] and tournament_result['semifinal2']:\n",
    "    print(f\"{tournament_result['semifinal1'][2]} vs {tournament_result['semifinal2'][2]}\")\n",
    "    \n",
    "print(\"\\nTournament Champion:\")\n",
    "print(tournament_result['champion'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}